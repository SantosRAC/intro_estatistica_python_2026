---
title: "Dia 05 - Análise multivariada: clustering e análise de componentes principais (PCA)"
subtitle: "Introdução à Estatística com Python"
author: "Renato A. Corrêa dos Santos"
date: "2025-12-13"
format:
  revealjs:
    theme: dracula
    transition: fade
    slide-number: true
    show-slide-number: all
    chalkboard: true
    width: 1280
    height: 720
    code-copy: true
    center: true
jupyter: python3
lang: pt-BR
execute:
  echo: true
  warning: false
  error: false
---

# Análise de agrupamento (Clustering)

:::: {.columns}

Conjunto de técnicas cuja finalidade é agregar objetos com base em suas características.

 - Agrupamentos se concentram nas características e são realizados com base em **distâncias**

::::

# Objetivos

:::: {.columns}

**Classes**, grupos ou clusters (agrupamento de objetos pela distância ou similaridade)

 - Obtenção de grupos de forma **mais homogênea possível**

 - Obtenção da **maior heterogeneidade possível fora** (comparação entre grupos)

::::

# A variável estatística de agrupamento

:::: {.columns}

 - Conjunto de variáveis que representam as características usadas para comparar objetos

::::



# Aplicações de análises de agrupamento


:::: {.columns}

Métodos hierárquicos são populares na bioinformática


::::

# Tipos de agrupamento

:::: {.columns}

 - Classificação **hierárquica**

 - Classificação **não hierárquica** (não há classificação de importância entre as classes)

Classificação é *independente das medidas de distância ou similaridade*.

::::

# Algoritmos (ou 'heurísticas') de agrupamento não hierárquicos

## K-means

:::: {.columns}

Algoritmo que **minimiza as distâncias entre os subgrupos e seus elementos**.

 - Não hierárquico

 - Processo iterativo

 - Necessário definir um *k* a priori


::::

## K-means - vantagens e desvantagens

:::: {.columns}

Vantagens

 - Computacionalmente simples

Desvatagens

 - Pesa a homogeneidade, deixando de lado a distância entre os grupos (inicialização ruim de centroides)

 - Má escolha de agrupamentos pode aproximar grupos ou participar grupos de forma artificial


::::

# Algoritmos hierárquicos

:::: {.columns}

Usa critérios de hierarquia para criar **relação entre elementos de cada grupo**.

Versões:

 - Aglomerativa

 - Divisiva

::::

## Algoritmo hierárquico aglomerativo

:::: {.columns}

Juntam-se elementos até então isolados.

- Exemplo usando distância euclidiana e representação matricial.

::::


# Funções de agrupamento

## Método do vizinho mais próximo (single-link, connected)

:::: {.columns}

A distância entre dois subgrupos é a menor distância entre os elementos do dois subgrupos.

 - Tendência de geração de classes com baixa definição

 - Computacionalmente menos custoso

::::

## Método do vizinho mais distante (complete-link, compact)

:::: {.columns}

A distância entre dois subgrupos é a maior distância entre os elementos do dois subgrupos.



::::

## Método das distâncias médias entre grupos (average-link, group average)

:::: {.columns}

A distância entre dois subgrupos é a média das distâncias entre os elementos de cada subgrupo.


::::

## Método da inércia mínima (Ward)

:::: {.columns}



::::



# Setup Inicial {background-color="#282a36"}


## Carregando Bibliotecas e Dados




# Análise de Componentes Principais (PCA)

:::: {.columns}

Técnica de redução de dimensionalidade

Amplamente usada:

 - Análise de dados
 - Aprendizado de máquina

::::


# Redução de dimensionalidade

:::: {.columns}

 - Identificação de componentes ortogonais que capturam a maior parte da variância no conjunto de dados inicial

 - Transformação de um conjunto de **features correlacionadas** em um **conjunto MENOR de features não correlacionadas**

::::


# Componentes Principais {.smaller}

:::: {.columns}

 Features linearmente não correlacionadas, cujos índices estão medindo diferentes dimensões dos dados:

 - **Primeira componente principal (PC1)**: direção no espaço de feature com maior variação dos dados.

 - **Segunda componente principal (PC2)**: direção no espaço de feature com maior variação dos dados, que seja ortogonal à PC1

 - **Demais componentes principais (PC3, PC4 ...)**: direção no espaço de feature com maior variação dos dados, que sejam ortogonais às componentes anteriores.

A maior parte da variação costuma ser descrita pelas primeiras CPs, cujas variâncias não são desprezíveis.

::::


# Quando usar

:::: {.columns}

 - Estão presentes **features correlacionadas** (correlação > 0,30)

 - Explora-se de pelo menos 70% da variância total da amostra 
 
 - É gerado um número mínimo de componentes principais

::::

## Etapas da PCA

:::: {.columns}


 - **Padronização** dos dados (PCA é afetada pela escala das variáveis/features)

 - Geração de uma **matriz de covariância**

 - **Decomposição espectral** - análise de autovalores e autovetores

 - **Ordenação de CPs** de maior para menor autovalor (valores maiores explicam a maior variância)


::::

## Aplicações de PCA

:::: {.columns}


 - **Visualização**

 - **Exploração**

 - **Pré-processamento**

::::

## PCA na prática

:::: {.columns}

```{python}
#| echo: true
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
```

::::

## Criando vetores de variáveis em dataframe {.smaller}

:::: {.columns}

```{python}
# gera um DataFrame com 20 variáveis conforme solicitado
rng = np.random.default_rng(12345)
n = 150

g1 = 6   # ~30% de 20
g2 = 8   # ~40% de 20
g3 = 20 - g1 - g2  # restante (6)

# sinais latentes independentes para cada grupo (garante alta correlação intra-grupo e baixa entre grupos)
z1 = rng.normal(0, 1, size=n)
z2 = rng.normal(5, 1.5, size=n)
```

::::

:::: {.columns}

```{python}
# grupo 1: altamente correlacionadas entre si
g1_vars = {f"G1_{i+1}": z1 + rng.normal(0, 0.1, size=n) for i in range(g1)}

# grupo 2: altamente correlacionadas entre si, pouco correlacionadas com grupo 1
g2_vars = {f"G2_{i+1}": z2 + rng.normal(0, 0.1, size=n) for i in range(g2)}

# variáveis restantes com distribuições aleatórias variadas
g3_vars = {
  "R_1": rng.uniform(-2, 2, size=n),
  "R_2": rng.exponential(1.0, size=n),
  "R_3": rng.poisson(3, size=n),
  "R_4": rng.normal(0, 1, size=n),
  "R_5": rng.binomial(1, 0.3, size=n),
  "R_6": rng.normal(2, 0.5, size=n),
}
```

::::

## Criando vetores de variáveis em dataframe {.smaller}

:::: {.columns}

```{python}
data = {**g1_vars, **g2_vars, **g3_vars}
df_pca = pd.DataFrame(data)
df_pca.head()
```

::::

## Criando vetores de variáveis em dataframe {.smaller}

:::: {.columns}

```{python}
df_pca.shape
```

::::

## Padronizando os dados {.smaller}

:::: {.columns}

```{python}
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_pca)
df_pca_scaled = pd.DataFrame(X_scaled, columns=df_pca.columns, index=df_pca.index)
df_pca_scaled.head()
```

::::

## Padronizando os dados {.smaller}

:::: {.columns}

```{python}
df_pca_scaled.shape
```

::::

## Calculando a matriz de covariância {.smaller}

:::: {.columns}

```{python}
cov_matrix = df_pca_scaled.cov()
cov_matrix.head(n = 8)
```

::::

## Decomposição espectral

 - **Autovalores**: variância explicada pelo componente
 
 - **Autovetores**: direção do componente

## Realizando a decomposição espectral - autovetores e autovalores {.smaller}

:::: {.columns}

::: {.column}

```{python}
import scipy as sp

eigvals, eigvecs = sp.linalg.eigh(cov_matrix)

# ordenar autovalores/autovetores em ordem decrescente
order = np.argsort(eigvals)[::-1]
eigvals = eigvals[order]
eigvecs = eigvecs[:, order]
```

:::

::::

## Autovetores e autovalores {.smaller}

:::: {.columns}

::: {.column}

```{python}
eigvals
```

:::

::: {.column}

```{python}
eigvecs
```

:::

::::

## Variância explicada {.smaller}

:::: {.columns}

```{python}
explained_var = eigvals / eigvals.sum()
explained_var
```

::::

## Obtendo a variância cumulativa (ordenando os PCs) {.smaller}

:::: {.columns}

```{python}
cum_explained = np.cumsum(explained_var)

explained_df = pd.DataFrame({
  "eigenvalue": eigvals,
  "explained_variance": explained_var,
  "cumulative_variance": cum_explained
}, index=[f"PC{i+1}" for i in range(len(eigvals))])

explained_df.head()
```

::::

## Projeção dos dados padronizados nas componentes principais {.smaller}

:::: {.columns}

```{python}
X_scaled
```

::::

## Projeção dos dados padronizados nas componentes principais {.smaller}

:::: {.columns}

```{python}
pcs = pd.DataFrame(X_scaled.dot(eigvecs), columns=explained_df.index, index=df_pca.index)
pcs.head()
```

::::

## Projeção dos dados padronizados nas componentes principais {.smaller}

:::: {.columns}

```{python}
#| fig-align: center
fig, axes = plt.subplots(1, 2, figsize=(10, 3))
axes[0].bar(range(1, len(explained_var)+1), explained_var * 100)
axes[0].set_xlabel("PC")
axes[0].set_ylabel("% Variância explicada")
axes[0].set_title("Scree plot")

axes[1].plot(range(1, len(cum_explained)+1), cum_explained * 100, marker='o')
axes[1].axhline(70, color='gray', ls='--')
axes[1].set_xlabel("Número de PCs")
axes[1].set_ylabel("Variância acumulada (%)")
axes[1].set_title("Variância acumulada")

plt.tight_layout()
plt.show()
```

::::

## Projeção dos dados padronizados nas componentes principais {.smaller}

:::: {.columns}

```{python}
#| fig-align: center
sns.scatterplot(x="PC1", y="PC2", data=pcs)
plt.title("PC1 vs PC2")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.show()
```

::::

## Explorando dados de pinguins

Importando dataset de pinguins:

```{python}
from palmerpenguins import load_penguins
penguins = load_penguins()
penguins.head()
```


Investigando as variáveis no conjunto de dados:

```{python}
penguins.columns
```


Selecionando apenas variáveis de interesse:

```{python}
vars_to_correlate = penguins[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']].head()
```


# Setup Inicial {background-color="#282a36"}


## Carregando Bibliotecas e Dados



::: {.callout-note}
**Nota:** XXXXX.
:::

# Uso de Inteligência Artificial

Material com exemplos criados com GitHub Copilot usando modelo(s):

 - `GPT 5.0`


# Referências

 - [PCA with penguins and recipes](https://allisonhorst.github.io/palmerpenguins/articles/pca.html)

 - Munonye, K. Data Science and Analytics with Python: A Comprehensive Guide. 2025.

 - Machado, C. C. e Campos, F. K. R. Análise multivariada: introdução aos conceitos. 2023.

