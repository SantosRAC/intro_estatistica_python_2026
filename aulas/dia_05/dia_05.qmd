---
title: "Dia 05 - Análise multivariada: clustering e análise de componentes principais (PCA)"
subtitle: "Introdução à Estatística com Python"
author: "Renato A. Corrêa dos Santos"
date: "2025-12-13"
format:
  revealjs:
    theme: dracula
    transition: fade
    slide-number: true
    show-slide-number: all
    chalkboard: true
    width: 1280
    height: 720
    code-copy: true
    center: true
jupyter: python3
lang: pt-BR
execute:
  echo: true
  warning: false
  error: false
---

# Análise supervisionada e não supervisionada

:::: {.columns}

 - **Supervisionada**: 

 - **Não supervisionada**: 

::::

# Importância de técnicas não supervisionadas

:::: {.columns}

 - Há uma grande quantidade de dados disponível para análise

 - Uma parte muito pequena dos dados está classificada

::::

# Análise de agrupamento (Clustering)

:::: {.columns}

Conjunto de técnicas cuja finalidade é agregar objetos com base em suas características.

 - Agrupamentos se concentram nas características e são realizados com base em **distâncias**

::::

# Objetivos

:::: {.columns}

**Classes**, grupos ou clusters (agrupamento de objetos pela distância ou similaridade)

 - Obtenção de grupos de forma **mais homogênea possível**

 - Obtenção da **maior heterogeneidade possível fora** (comparação entre grupos)

::::

# A variável estatística de agrupamento

:::: {.columns}

 - Conjunto de variáveis que representam as características usadas para comparar objetos

::::



# Aplicações de análises de agrupamento


:::: {.columns}

Métodos hierárquicos são populares na bioinformática


::::

# Tipos de agrupamento

:::: {.columns}

 - Classificação **hierárquica**

 - Classificação **não hierárquica** (não há classificação de importância entre as classes)

Classificação é *independente das medidas de distância ou similaridade*.

::::

# Algoritmos (ou 'heurísticas') de agrupamento não hierárquicos

## K-means

:::: {.columns}

Também chamado de **algoritmo de Lloyd-Forgy**

Algoritmo que **minimiza as distâncias entre os subgrupos e seus elementos**.

 - Não hierárquico

 - Processo iterativo

 - Necessário definir um *k* a priori


::::

## K-means - vantagens e desvantagens

:::: {.columns}

Vantagens

 - Computacionalmente simples

Desvatagens

 - Pesa a homogeneidade, deixando de lado a distância entre os grupos (inicialização ruim de centroides)

 - Má escolha de agrupamentos pode aproximar grupos ou particionar grupos de forma artificial

 - Método não tem bom comportamento quando grupos têm diâmetros muito diferentes

::::


## K-means: simulando um dataset em Python

:::: {.columns}

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
```

::::

## K-means: simulando um dataset em Python {.smaller}

:::: {.columns}

::: {.column}

```{python}
#| fig-align: center
X, y_true = make_blobs(n_samples=300,
             centers=[(-5, -2), (0, 5), (5, 0), (3, -4)],
             cluster_std=[0.6, 1.0, 0.5, 0.8],
             random_state=42)

exemplo_kmeans_df = pd.DataFrame(X, columns=["x", "y"])
exemplo_kmeans_df["true_label"] = y_true

X
```

:::

::: {.column}

```{python}
exemplo_kmeans_df.head()
```

:::

::::

## K-means: simulando um dataset em Python {.smaller}

:::: {.columns}

```{python}
#| fig-align: center
plt.figure(figsize=(5,3))
plt.scatter(exemplo_kmeans_df["x"], exemplo_kmeans_df["y"], cmap="tab10", s=1)
plt.title("Dados simulados (rótulos verdadeiros)")
plt.xlabel("x"); plt.ylabel("y")
plt.show()
```

::::

## Executando o algoritmo K-means {.smaller}

:::: {.columns}

 - Escolha do k (aparentemente temos 4 grupos)

```{python}
k = 4
km = KMeans(n_clusters=k, random_state=42, n_init=10)
labels = km.fit_predict(X)
```

::::

## Inspecionando centroides {.smaller}

:::: {.columns}

```{python}
centroids = km.cluster_centers_

centroids
```

::::

## Inspecionando labels {.smaller}

:::: {.columns}

 - **Atenção:** "Labels" (ou etiquetas) em agrupamentos com métodos não supervisionados não devem ser confundidos com labels de classes de métodos supervisionados!

```{python}
print(labels)

exemplo_kmeans_df["kmeans_label"] = labels

print(exemplo_kmeans_df.head())
```

::::

## Métricas para avaliar os resultados {.smaller}

 - **Hard clustering**: atribuindo pontos a um único cluster

 - **Soft clustering**: atribuindo pontos a múltiplos clusters

   - Métricas como a distância de pontos a diferentes centroides podem ajudar a entender os agrupamentos

```{python}
km.transform(X).round(2)
```

## Métricas para avaliar os resultados {.smaller}

 - **Silhouette Score**: 

```{python}
# métricas e visualização do resultado
print("Inércia (soma de quadrados intra-cluster):", km.inertia_)
print("Silhouette score:", silhouette_score(X, labels))
```

## K-means em Python {.smaller}

:::: {.columns}

```{python}
#| fig-align: center
plt.figure(figsize=(6,4))
plt.scatter(exemplo_kmeans_df["x"], exemplo_kmeans_df["y"], c=exemplo_kmeans_df["kmeans_label"], cmap="tab10", s=30, alpha=0.8)
plt.scatter(centroids[:,0], centroids[:,1], c="black", s=100, marker="x")
plt.title("K-means (k=4) - clusters e centroides")
plt.xlabel("x"); plt.ylabel("y")
plt.show()
```

::::

## K-means: simulando um dataset em Python {.smaller}

:::: {.columns}

 - **Atribuição de clusters** de acordo com o algoritmo de simulação

```{python}
#| fig-align: center
plt.figure(figsize=(5,3))
plt.scatter(exemplo_kmeans_df["x"], exemplo_kmeans_df["y"], c=exemplo_kmeans_df["true_label"], cmap="tab10", s=1)
plt.title("Dados simulados (rótulos verdadeiros)")
plt.xlabel("x"); plt.ylabel("y")
plt.show()
```

::::

## Otimizações de K-means

:::: {.columns}

 - É possível alterar o **método de inicialização de centroides**

 - Estratégias para identificar o **número ótimo de *k***

   - Inertia vs. k (cotovelo)

   - Silhouette score vs. k

::::

# Algoritmos hierárquicos

:::: {.columns}

Usa critérios de hierarquia para criar **relação entre elementos de cada grupo**.

Versões:

 - Aglomerativa

 - Divisiva

::::

## Algoritmo hierárquico aglomerativo

:::: {.columns}

Juntam-se elementos até então isolados.

- Exemplo usando distância euclidiana e representação matricial.

::::


# Funções de agrupamento

## Método do vizinho mais próximo (single-link, connected)

:::: {.columns}

A distância entre dois subgrupos é a menor distância entre os elementos do dois subgrupos.

 - Tendência de geração de classes com baixa definição

 - Computacionalmente menos custoso

::::

## Método do vizinho mais distante (complete-link, compact)

:::: {.columns}

A distância entre dois subgrupos é a maior distância entre os elementos do dois subgrupos.



::::

## Método das distâncias médias entre grupos (average-link, group average)

:::: {.columns}

A distância entre dois subgrupos é a média das distâncias entre os elementos de cada subgrupo.


::::

## Método da inércia mínima (Ward)

:::: {.columns}



::::

# Análises de agrupamento com os dados de pinguins

## Explorando dados de pinguins

:::: {.columns}

Importando dataset de pinguins:

```{python}
from palmerpenguins import load_penguins
import seaborn as sns
penguins = load_penguins()
penguins.head()
```

::::

## Visualizando os dados {.smaller}

:::: {.columns}

```{python}
#| fig-align: center
penguins_subset = penguins[['species', 'bill_length_mm', 'bill_depth_mm',
       'flipper_length_mm', 'body_mass_g']]
sns.pairplot(penguins_subset, hue='species', height=1.5)
```

::::

## Agrupando os dados de pinguins (comprimento bico) {.smaller}

:::: {.columns}

```{python}
penguins_subset_bill_length = penguins[['species', 'bill_length_mm']]
print(penguins_subset_bill_length[['bill_length_mm']])
```

::::

## Dados faltantes no dataset de pinguins (comprimento bico) {.smaller}

:::: {.columns}

 - Método de K-means não aceita dados faltantes! (sim, o dia a dia é cheio de exceções rs)

```{python}
print("Dimensões do dataframe depois:\n", penguins_subset_bill_length.shape)
na_counts = penguins_subset_bill_length.isna().sum()
print("Dados faltantes por coluna:\n", na_counts)
penguins_subset_bill_length = penguins_subset_bill_length.dropna(subset=["bill_length_mm"])
print("Dimensões do dataframe depois:\n", penguins_subset_bill_length.shape)
```

::::

## Agrupando os dados de pinguins (comprimento bico) {.smaller}

:::: {.columns}

 - Usando **4** como valor de *k* 

```{python}
k = 4
km = KMeans(n_clusters=k, random_state=42, n_init=10)
bill_length_mm_pred = km.fit_predict(penguins_subset_bill_length[['bill_length_mm']])
penguins_subset_bill_length["kmeans_label"] = bill_length_mm_pred
```

::::

## Agrupando os dados de pinguins (comprimento bico) {.smaller}

:::: {.columns}

```{python}
centroids = km.cluster_centers_
centroids
```

::::

## Visualizando os grupos (comprimento bico) {.smaller}

:::: {.columns}

```{python}
#| fig-align: center
plt.figure(figsize=(6,4))
plt.scatter(penguins_subset_bill_length["bill_length_mm"], penguins_subset_bill_length["bill_length_mm"], c=penguins_subset_bill_length["kmeans_label"], cmap="tab10", s=30, alpha=0.8)
plt.scatter(centroids, centroids, c="black", s=100, marker="x")
plt.title("K-means (k=4) - clusters e centroides")
plt.xlabel("Comprimento de bico (mm)"); plt.ylabel("Comprimento de bico (mm)")
plt.show()
```

::::

## Escolha de k para comprimento bico {.smaller}

:::: {.columns}

```{python}
#| fig-align: center
X = penguins_subset_bill_length[['bill_length_mm']].values
ks = range(2, 11)
scores = []
for k in ks:
  km = KMeans(n_clusters=k, random_state=42, n_init=10)
  labels = km.fit_predict(X)
  scores.append(silhouette_score(X, labels))

plt.figure(figsize=(6,3))
plt.plot(list(ks), scores, marker='o')
plt.xticks(list(ks))
plt.xlabel('k')
plt.ylabel('Silhouette score')
plt.title('Silhouette score vs k (bill_length_mm)')
plt.grid(True)
plt.show()
```

::::

## Agrupando os dados de pinguins (bico) {.smaller}

:::: {.columns}

```{python}
penguins_subset_bill_info = penguins[['species', 'bill_length_mm', 'bill_depth_mm']]
print(penguins_subset_bill_info[['bill_length_mm', 'bill_depth_mm']])
```

::::

## Dados faltantes no dataset de pinguins (bico) {.smaller}

:::: {.columns}

```{python}
print("Dimensões do dataframe depois:\n", penguins_subset_bill_info.shape)
na_counts = penguins_subset_bill_info.isna().sum()
print("Dados faltantes por coluna:\n", na_counts)
penguins_subset_bill_info = penguins_subset_bill_info.dropna(subset=["bill_length_mm", "bill_depth_mm"])
print("Dimensões do dataframe depois:\n", penguins_subset_bill_info.shape)
```

::::

## Escolha de k para dados de bico {.smaller}

:::: {.columns}

```{python}
#| fig-align: center
X = penguins_subset_bill_info[['bill_length_mm', 'bill_depth_mm']].values
ks = range(2, 11)
scores = []
for k in ks:
  km = KMeans(n_clusters=k, random_state=42, n_init=10)
  labels = km.fit_predict(X)
  scores.append(silhouette_score(X, labels))

plt.figure(figsize=(6,3))
plt.plot(list(ks), scores, marker='o')
plt.xticks(list(ks))
plt.xlabel('k')
plt.ylabel('Silhouette score')
plt.title('Silhouette score vs k (bill_length_mm & bill_depth_mm)')
plt.grid(True)
plt.show()
```

::::

## Agrupando os dados de pinguins (bico) {.smaller}

:::: {.columns}

 - Usando **3** como valor de *k* 

```{python}
k = 3
km = KMeans(n_clusters=k, random_state=42, n_init=10)
penguins_subset_bill_info_pred = km.fit_predict(penguins_subset_bill_info[['bill_length_mm', 'bill_depth_mm']])
penguins_subset_bill_info["kmeans_label"] = penguins_subset_bill_info_pred

centroids = km.cluster_centers_
centroids
```

::::

## Visualizando os grupos (bico) {.smaller}

:::: {.columns}

::: {.column}

```{python}
#| fig-align: center
plt.figure(figsize=(6,4))
plt.scatter(penguins_subset_bill_info["bill_length_mm"], penguins_subset_bill_info["bill_depth_mm"], c=penguins_subset_bill_info["kmeans_label"], cmap="tab10", s=30, alpha=0.8)
plt.scatter(centroids[:,0], centroids[:,1], c="black", s=100, marker="x")
plt.title("K-means (k=3) - clusters e centroides")
plt.xlabel("Comprimento de bico"); plt.ylabel("Profundidade de bico")
plt.show()
```

:::

::: {.column}

```{python}
#| fig-align: center
plt.figure(figsize=(5,3))
sns.scatterplot(
  data=penguins_subset_bill_info,
  x="bill_length_mm",
  y="bill_depth_mm",
  hue="species",
  palette="tab10",
  s=40,
  alpha=0.8
)
plt.title("Distribuição por espécie (bico)")
plt.xlabel("Comprimento de bico")
plt.ylabel("Profundidade de bico")
plt.show()
```

:::

::::

## Agrupando os dados de pinguins (bico) {.smaller}

:::: {.columns}

 - Usando **2** como valor de *k* 

```{python}
k = 2
km = KMeans(n_clusters=k, random_state=42, n_init=10)
penguins_subset_bill_info_pred = km.fit_predict(penguins_subset_bill_info[['bill_length_mm', 'bill_depth_mm']])
penguins_subset_bill_info["kmeans_label"] = penguins_subset_bill_info_pred

centroids = km.cluster_centers_
centroids
```

::::

## Visualizando os grupos (bico) {.smaller}

:::: {.columns}

::: {.column}

```{python}
#| fig-align: center
plt.figure(figsize=(6,4))
plt.scatter(penguins_subset_bill_info["bill_length_mm"], penguins_subset_bill_info["bill_depth_mm"], c=penguins_subset_bill_info["kmeans_label"], cmap="tab10", s=30, alpha=0.8)
plt.scatter(centroids[:,0], centroids[:,1], c="black", s=100, marker="x")
plt.title("K-means (k=2) - clusters e centroides")
plt.xlabel("Comprimento de bico"); plt.ylabel("Profundidade de bico")
plt.show()
```

:::

::: {.column}

```{python}
#| fig-align: center
plt.figure(figsize=(5,3))
sns.scatterplot(
  data=penguins_subset_bill_info,
  x="bill_length_mm",
  y="bill_depth_mm",
  hue="species",
  palette="tab10",
  s=40,
  alpha=0.8
)
plt.title("Distribuição por espécie (bico)")
plt.xlabel("Comprimento de bico")
plt.ylabel("Profundidade de bico")
plt.show()
```

:::

::::

# Análise de Componentes Principais (PCA)

:::: {.columns}

Técnica de redução de dimensionalidade

Amplamente usada:

 - Análise de dados
 - Aprendizado de máquina

::::


# Redução de dimensionalidade

:::: {.columns}

 - Identificação de componentes ortogonais que capturam a maior parte da variância no conjunto de dados inicial

 - Transformação de um conjunto de **features correlacionadas** em um **conjunto MENOR de features não correlacionadas**

::::


# Componentes Principais {.smaller}

:::: {.columns}

 Features linearmente não correlacionadas, cujos índices estão medindo diferentes dimensões dos dados:

 - **Primeira componente principal (PC1)**: direção no espaço de feature com maior variação dos dados.

 - **Segunda componente principal (PC2)**: direção no espaço de feature com maior variação dos dados, que seja ortogonal à PC1

 - **Demais componentes principais (PC3, PC4 ...)**: direção no espaço de feature com maior variação dos dados, que sejam ortogonais às componentes anteriores.

A maior parte da variação costuma ser descrita pelas primeiras CPs, cujas variâncias não são desprezíveis.

::::


# Quando usar

:::: {.columns}

 - Estão presentes **features correlacionadas** (correlação > 0,30)

 - Explora-se de pelo menos 70% da variância total da amostra 
 
 - É gerado um número mínimo de componentes principais

::::

## Etapas da PCA

:::: {.columns}


 - **Padronização** dos dados (PCA é afetada pela escala das variáveis/features)

 - Geração de uma **matriz de covariância**

 - **Decomposição espectral** - análise de autovalores e autovetores

 - **Ordenação de CPs** de maior para menor autovalor (valores maiores explicam a maior variância)


::::

## Aplicações de PCA

:::: {.columns}


 - **Visualização**

 - **Exploração**

 - **Pré-processamento**

::::

# PCA na prática

## Criando vetores de variáveis em dataframe {.smaller}

:::: {.columns}

```{python}
# gera um DataFrame com 20 variáveis conforme solicitado
rng = np.random.default_rng(12345)
n = 150

g1 = 6   # ~30% de 20
g2 = 8   # ~40% de 20
g3 = 20 - g1 - g2  # restante (6)

# sinais latentes independentes para cada grupo (garante alta correlação intra-grupo e baixa entre grupos)
z1 = rng.normal(0, 1, size=n)
z2 = rng.normal(5, 1.5, size=n)
```

::::

:::: {.columns}

```{python}
# grupo 1: altamente correlacionadas entre si
g1_vars = {f"G1_{i+1}": z1 + rng.normal(0, 0.1, size=n) for i in range(g1)}

# grupo 2: altamente correlacionadas entre si, pouco correlacionadas com grupo 1
g2_vars = {f"G2_{i+1}": z2 + rng.normal(0, 0.1, size=n) for i in range(g2)}

# variáveis restantes com distribuições aleatórias variadas
g3_vars = {
  "R_1": rng.uniform(-2, 2, size=n),
  "R_2": rng.exponential(1.0, size=n),
  "R_3": rng.poisson(3, size=n),
  "R_4": rng.normal(0, 1, size=n),
  "R_5": rng.binomial(1, 0.3, size=n),
  "R_6": rng.normal(2, 0.5, size=n),
}
```

::::

## Criando vetores de variáveis em dataframe {.smaller}

:::: {.columns}

```{python}
data = {**g1_vars, **g2_vars, **g3_vars}
df_pca = pd.DataFrame(data)
df_pca.head()
```

::::

## Criando vetores de variáveis em dataframe {.smaller}

:::: {.columns}

```{python}
df_pca.shape
```

::::

## Padronizando os dados {.smaller}

:::: {.columns}

```{python}
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_pca)
df_pca_scaled = pd.DataFrame(X_scaled, columns=df_pca.columns, index=df_pca.index)
df_pca_scaled.head()
```

::::

## Padronizando os dados {.smaller}

:::: {.columns}

```{python}
df_pca_scaled.shape
```

::::

## Calculando a matriz de covariância {.smaller}

:::: {.columns}

```{python}
cov_matrix = df_pca_scaled.cov()
cov_matrix.head(n = 8)
```

::::

## Decomposição espectral

 - **Autovalores**: variância explicada pelo componente
 
 - **Autovetores**: direção do componente

## Realizando a decomposição espectral - autovetores e autovalores {.smaller}

:::: {.columns}

::: {.column}

```{python}
import scipy as sp

eigvals, eigvecs = sp.linalg.eigh(cov_matrix)

# ordenar autovalores/autovetores em ordem decrescente
order = np.argsort(eigvals)[::-1]
eigvals = eigvals[order]
eigvecs = eigvecs[:, order]
```

:::

::::

## Autovetores e autovalores {.smaller}

:::: {.columns}

::: {.column}

```{python}
eigvals
```

:::

::: {.column}

```{python}
eigvecs
```

:::

::::

## Variância explicada {.smaller}

:::: {.columns}

```{python}
explained_var = eigvals / eigvals.sum()
explained_var
```

::::

## Obtendo a variância cumulativa (ordenando os PCs) {.smaller}

:::: {.columns}

```{python}
cum_explained = np.cumsum(explained_var)

explained_df = pd.DataFrame({
  "eigenvalue": eigvals,
  "explained_variance": explained_var,
  "cumulative_variance": cum_explained
}, index=[f"PC{i+1}" for i in range(len(eigvals))])

explained_df.head()
```

::::

## Projeção dos dados padronizados nas componentes principais {.smaller}

:::: {.columns}

```{python}
X_scaled
```

::::

## Projeção dos dados padronizados nas componentes principais {.smaller}

:::: {.columns}

```{python}
pcs = pd.DataFrame(X_scaled.dot(eigvecs), columns=explained_df.index, index=df_pca.index)
pcs.head()
```

::::

## Projeção dos dados padronizados nas componentes principais {.smaller}

:::: {.columns}

```{python}
#| fig-align: center
fig, axes = plt.subplots(1, 2, figsize=(10, 3))
axes[0].bar(range(1, len(explained_var)+1), explained_var * 100)
axes[0].set_xlabel("PC")
axes[0].set_ylabel("% Variância explicada")
axes[0].set_title("Scree plot")

axes[1].plot(range(1, len(cum_explained)+1), cum_explained * 100, marker='o')
axes[1].axhline(70, color='gray', ls='--')
axes[1].set_xlabel("Número de PCs")
axes[1].set_ylabel("Variância acumulada (%)")
axes[1].set_title("Variância acumulada")

plt.tight_layout()
plt.show()
```

::::

## Projeção dos dados padronizados nas componentes principais {.smaller}

:::: {.columns}

```{python}
#| fig-align: center
sns.scatterplot(x="PC1", y="PC2", data=pcs)
plt.title("PC1 vs PC2")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.show()
```

::::

# Uso de Inteligência Artificial

Material com exemplos criados com GitHub Copilot usando modelo(s):

 - `GPT 5.0`


# Referências

 - [PCA with penguins and recipes](https://allisonhorst.github.io/palmerpenguins/articles/pca.html)

 - Géron, A. Hands-On Machine Learning with Scikit-Learn, Keras & Tensorflow. Terceira Edição (capítulo 9). 2025.

 - Munonye, K. Data Science and Analytics with Python: A Comprehensive Guide. 2025.

 - Machado, C. C. e Campos, F. K. R. Análise multivariada: introdução aos conceitos. 2023.

