---
title: "Dia 05 - Análise multivariada: clustering e análise de componentes principais (PCA)"
subtitle: "Introdução à Estatística com Python"
author: "Renato A. Corrêa dos Santos"
date: "2025-12-13"
format:
  revealjs:
    theme: dracula
    transition: fade
    slide-number: true
    show-slide-number: all
    chalkboard: true
    width: 1280
    height: 720
    code-copy: true
    center: true
jupyter: python3
lang: pt-BR
execute:
  echo: true
  warning: false
  error: false
---

# Análise de agrupamento (Clustering)

:::: {.columns}

Conjunto de técnicas cuja finalidade é agregar objetos com base em suas características.

 - Agrupamentos se concentram nas características e são realizados com base em **distâncias**

::::

# Objetivos

:::: {.columns}

**Classes**, grupos ou clusters (agrupamento de objetos pela distância ou similaridade)

 - Obtenção de grupos de forma **mais homogênea possível**

 - Obtenção da **maior heterogeneidade possível fora** (comparação entre grupos)

::::

# A variável estatística de agrupamento

:::: {.columns}

 - Conjunto de variáveis que representam as características usadas para comparar objetos

::::



# Aplicações de análises de agrupamento


:::: {.columns}

Métodos hierárquicos são populares na bioinformática


::::

# Tipos de agrupamento

:::: {.columns}

 - Classificação **hierárquica**

 - Classificação **não hierárquica** (não há classificação de importância entre as classes)

Classificação é *independente das medidas de distância ou similaridade*.

::::

# Algoritmos (ou 'heurísticas') de agrupamento não hierárquicos

## K-means

:::: {.columns}

Algoritmo que **minimiza as distâncias entre os subgrupos e seus elementos**.

 - Não hierárquico

 - Processo iterativo

 - Necessário definir um *k* a priori


::::

## K-means - vantagens e desvantagens

:::: {.columns}

Vantagens

 - Computacionalmente simples

Desvatagens

 - Pesa a homogeneidade, deixando de lado a distância entre os grupos (inicialização ruim de centroides)

 - Má escolha de agrupamentos pode aproximar grupos ou participar grupos de forma artificial


::::

# Algoritmos hierárquicos

:::: {.columns}

Usa critérios de hierarquia para criar **relação entre elementos de cada grupo**.

Versões:

 - Aglomerativa

 - Divisiva

::::

## Algoritmo hierárquico aglomerativo

:::: {.columns}

Juntam-se elementos até então isolados.

- Exemplo usando distância euclidiana e representação matricial.

::::


# Funções de agrupamento

## Método do vizinho mais próximo (single-link, connected)

:::: {.columns}

A distância entre dois subgrupos é a menor distância entre os elementos do dois subgrupos.

 - Tendência de geração de classes com baixa definição

 - Computacionalmente menos custoso

::::

## Método do vizinho mais distante (complete-link, compact)

:::: {.columns}

A distância entre dois subgrupos é a maior distância entre os elementos do dois subgrupos.



::::

## Método das distâncias médias entre grupos (average-link, group average)

:::: {.columns}

A distância entre dois subgrupos é a média das distâncias entre os elementos de cada subgrupo.


::::

## Método da inércia mínima (Ward)

:::: {.columns}



::::



# Setup Inicial {background-color="#282a36"}


## Carregando Bibliotecas e Dados




# Análise de Componentes Principais (PCA)

:::: {.columns}

Técnica de redução de dimensionalidade

Amplamente usada:

 - Análise de dados
 - Aprendizado de máquina

::::


# Redução de dimensionalidade

:::: {.columns}

 - Identificação de componentes ortogonais que capturam a maior parte da variância no conjunto de dados inicial

 - Transformação de um conjunto de **features correlacionadas** em um **conjunto MENOR de features não correlacionadas**

::::


# Componentes Principais {.smaller}

:::: {.columns}

 Features linearmente não correlacionadas, cujos índices estão medindo diferentes dimensões dos dados:

 - **Primeira componente principal (PC1)**: direção no espaço de feature com maior variação dos dados.

 - **Segunda componente principal (PC2)**: direção no espaço de feature com maior variação dos dados, que seja ortogonal à PC1

 - **Demais componentes principais (PC3, PC4 ...)**: direção no espaço de feature com maior variação dos dados, que sejam ortogonais às componentes anteriores.

A maior parte da variação costuma ser descrita pelas primeiras CPs, cujas variâncias não são desprezíveis.

::::


# Quando usar

:::: {.columns}

 - Estão presentes **features correlacionadas** (correlação > 0,30)

 - Explora-se de pelo menos 70% da variância total da amostra 
 
 - É gerado um número mínimo de componentes principais

::::

## Etapas da PCA

:::: {.columns}


 - **Padronização** dos dados (PCA é afetada pela escala das variáveis/features)

 - Geração de uma **matriz de covariância**

 - **Decomposição espectral** - análise de autovalores e autovetores

 - **Ordenação de CPs** de maior para menor autovalor (valores maiores explicam a maior variância)



::::

## Aplicações de PCA

:::: {.columns}


 - **Visualização**

 - **Exploração**

 - **Pré-processamento**



::::

# Setup Inicial {background-color="#282a36"}


## Carregando Bibliotecas e Dados


```{python}
#| echo: true
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
```


::: {.callout-note}
**Nota:** XXXXX.
:::

# Uso de Inteligência Artificial

Material com exemplos criados com GitHub Copilot usando modelo(s):

 - `GPT 5.0`


# Referências

 - Munonye, K. Data Science and Analytics with Python: A Comprehensive Guide. 2025.

 - Machado, C. C. e Campos, F. K. R. Análise multivariada: introdução aos conceitos. 2023.

