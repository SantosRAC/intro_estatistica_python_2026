---
title: "Dia 05 - Análise multivariada: clustering e análise de componentes principais (PCA)"
subtitle: "Introdução à Estatística com Python"
author: "Renato A. Corrêa dos Santos"
date: "2026-02-13"
format:
  revealjs:
    theme: dracula
    transition: fade
    slide-number: true
    show-slide-number: all
    chalkboard: true
    width: 1280
    height: 720
    code-copy: true
    center: true
jupyter: python3
lang: pt-BR
execute:
  echo: true
  warning: false
  error: false
---

# Análise de agrupamento (Clustering)

:::: {.columns}

Conjunto de técnicas cuja finalidade é agregar objetos com base em suas características.

 - Agrupamentos se concentram nas características e são realizados com base em **distâncias**

::::

# Objetivos

:::: {.columns}

**Classes**, grupos ou clusters (agrupamento de objetos pela distância ou similaridade)

 - Obtenção de grupos de forma **mais homogênea possível**

 - Obtenção da **maior heterogeneidade possível fora** (comparação entre grupos)

::::

## A variável estatística de agrupamento

:::: {.columns}

 - Conjunto de variáveis que representam as características usadas para comparar objetos

::::

# Tipos de agrupamento

:::: {.columns}

 - Classificação **hierárquica**: há classificação de importância entre as classes

 - Classificação **não hierárquica**: não há classificação de importância entre as classes

Classificação é *independente das medidas de distância ou similaridade*.

::::

## Algoritmo K-means

:::: {.columns}

Também chamado de **algoritmo de Lloyd-Forgy**

Algoritmo que **minimiza as distâncias entre os subgrupos e seus elementos**.

 - Não hierárquico

 - Processo iterativo

 - Necessário definir um *k* a priori


::::

## K-means - vantagens e desvantagens

:::: {.columns}

Vantagens

 - Computacionalmente simples

Desvatagens

 - Pesa a homogeneidade, deixando de lado a distância entre os grupos (inicialização ruim de centroides)

 - Má escolha de agrupamentos pode aproximar grupos ou particionar grupos de forma artificial

 - Método não tem bom comportamento quando grupos têm diâmetros muito diferentes

::::

## Importando bibliotecas

:::: {.columns}

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
```

::::

# Análises de agrupamento com os dados de pinguins

## Explorando dados de pinguins

:::: {.columns}

Importando dataset de pinguins:

```{python}
from palmerpenguins import load_penguins
import seaborn as sns
penguins = load_penguins()
penguins.head()
```

::::

## Visualizando os dados {.smaller}

:::: {.columns}

```{python}
#| fig-align: center
penguins_subset = penguins[['species', 'bill_length_mm', 'bill_depth_mm',
       'flipper_length_mm', 'body_mass_g']]
sns.pairplot(penguins_subset, hue='species', height=1.5)
```

::::

## Agrupando os dados de pinguins (comprimento bico) {.smaller}

:::: {.columns}

```{python}
penguins_subset_bill_length = penguins[['species', 'bill_length_mm']]
print(penguins_subset_bill_length[['bill_length_mm']])
```

::::

## Dados faltantes no dataset de pinguins (comprimento bico) {.smaller}

:::: {.columns}

 - Método de K-means não aceita dados faltantes! (sim, o dia a dia é cheio de exceções rs)

```{python}
print("Dimensões do dataframe depois:\n", penguins_subset_bill_length.shape)
na_counts = penguins_subset_bill_length.isna().sum()
print("Dados faltantes por coluna:\n", na_counts)
penguins_subset_bill_length = penguins_subset_bill_length.dropna(subset=["bill_length_mm"])
print("Dimensões do dataframe depois:\n", penguins_subset_bill_length.shape)
```

::::

## Agrupando os dados de pinguins (comprimento bico) {.smaller}

:::: {.columns}

 - Usando **4** como valor de *k* 

```{python}
k = 4
km = KMeans(n_clusters=k, random_state=42, n_init=10)
bill_length_mm_pred = km.fit_predict(penguins_subset_bill_length[['bill_length_mm']])
penguins_subset_bill_length["kmeans_label"] = bill_length_mm_pred
```

::::

## Agrupando os dados de pinguins (comprimento bico) {.smaller}

:::: {.columns}

```{python}
centroids = km.cluster_centers_
print("Centroides:\n", centroids)
```

::::

## Inspecionando labels {.smaller}

:::: {.columns}

 - **Atenção:** "Labels" (ou etiquetas) em agrupamentos com métodos não supervisionados não devem ser confundidos com labels de classes de métodos supervisionados!

```{python}
labels = km.fit_predict(penguins_subset_bill_length[['bill_length_mm']])

penguins_subset_bill_length["kmeans_label"] = labels

print(penguins_subset_bill_length.head())
```

::::

## Agrupando os dados de pinguins (comprimento bico) {.smaller}

:::: {.columns}

```{python}
centroids = km.cluster_centers_
centroids
```

::::

## Visualizando os grupos (comprimento bico) {.smaller}

:::: {.columns}

```{python}
#| fig-align: center
plt.figure(figsize=(6,4))
plt.scatter(penguins_subset_bill_length["bill_length_mm"], penguins_subset_bill_length["bill_length_mm"], c=penguins_subset_bill_length["kmeans_label"], cmap="tab10", s=30, alpha=0.8)
plt.scatter(centroids, centroids, c="black", s=100, marker="x")
plt.title("K-means (k=4) - clusters e centroides")
plt.xlabel("Comprimento de bico (mm)"); plt.ylabel("Comprimento de bico (mm)")
plt.show()
```

::::

## Métricas para avaliar os resultados de agrupamento {.smaller}
 
 - **Inércia**: quanto menor, melhor o agrupamento.
 - **Silhouette** (0 a 1): valor próximo de 1 indica que amostra está distante do cluster vizinho.
 - **Silhouette Score**: valor médio dos coeficientes Silhouette de cada amostra; valores próximos de 1 indicam bom agrupamento.

```{python}
# métricas e visualização do resultado
print("Inércia (soma de quadrados intra-cluster):", km.inertia_)
print("Silhouette score:", silhouette_score(penguins_subset_bill_length[['bill_length_mm']], labels))
```

## Escolha de k para comprimento bico {.smaller}

:::: {.columns}

```{python}
#| fig-align: center
X = penguins_subset_bill_length[['bill_length_mm']].values
ks = range(2, 11)
scores = []
for k in ks:
  km = KMeans(n_clusters=k, random_state=42, n_init=10)
  labels = km.fit_predict(X)
  scores.append(silhouette_score(X, labels))

plt.figure(figsize=(6,3))
plt.plot(list(ks), scores, marker='o')
plt.xticks(list(ks))
plt.xlabel('k')
plt.ylabel('Silhouette score')
plt.title('Silhouette score vs k (bill_length_mm)')
plt.grid(True)
plt.show()
```

::::

## Agrupando os dados de pinguins (bico) {.smaller}

:::: {.columns}

```{python}
penguins_subset_bill_info = penguins[['species', 'bill_length_mm', 'bill_depth_mm']]
print(penguins_subset_bill_info[['bill_length_mm', 'bill_depth_mm']])
```

::::

## Dados faltantes no dataset de pinguins (bico) {.smaller}

:::: {.columns}

```{python}
print("Dimensões do dataframe depois:\n", penguins_subset_bill_info.shape)
na_counts = penguins_subset_bill_info.isna().sum()
print("Dados faltantes por coluna:\n", na_counts)
penguins_subset_bill_info = penguins_subset_bill_info.dropna(subset=["bill_length_mm", "bill_depth_mm"])
print("Dimensões do dataframe depois:\n", penguins_subset_bill_info.shape)
```

::::

## Escolha de k para dados de bico {.smaller}

:::: {.columns}

```{python}
#| fig-align: center
X = penguins_subset_bill_info[['bill_length_mm', 'bill_depth_mm']].values
ks = range(2, 11)
scores = []
for k in ks:
  km = KMeans(n_clusters=k, random_state=42, n_init=10)
  labels = km.fit_predict(X)
  scores.append(silhouette_score(X, labels))

plt.figure(figsize=(6,3))
plt.plot(list(ks), scores, marker='o')
plt.xticks(list(ks))
plt.xlabel('k')
plt.ylabel('Silhouette score')
plt.title('Silhouette score vs k (bill_length_mm & bill_depth_mm)')
plt.grid(True)
plt.show()
```

::::

## Agrupando os dados de pinguins (bico) {.smaller}

:::: {.columns}

 - Usando **3** como valor de *k* 

```{python}
k = 3
km = KMeans(n_clusters=k, random_state=42, n_init=10)
penguins_subset_bill_info_pred = km.fit_predict(penguins_subset_bill_info[['bill_length_mm', 'bill_depth_mm']])
penguins_subset_bill_info["kmeans_label"] = penguins_subset_bill_info_pred

centroids = km.cluster_centers_
centroids
```

::::

## Métricas de agrupamento {.smaller}

```{python}
# métricas e visualização do resultado
print("Inércia (soma de quadrados intra-cluster):", km.inertia_)
print("Silhouette score:", silhouette_score(penguins_subset_bill_info[['bill_length_mm', 'bill_depth_mm']], penguins_subset_bill_info["kmeans_label"]))
```

## Visualizando os grupos (bico) {.smaller}

:::: {.columns}

::: {.column}

```{python}
#| fig-align: center
plt.figure(figsize=(6,4))
plt.scatter(penguins_subset_bill_info["bill_length_mm"], penguins_subset_bill_info["bill_depth_mm"], c=penguins_subset_bill_info["kmeans_label"], cmap="tab10", s=30, alpha=0.8)
plt.scatter(centroids[:,0], centroids[:,1], c="black", s=100, marker="x")
plt.title("K-means (k=3) - clusters e centroides")
plt.xlabel("Comprimento de bico"); plt.ylabel("Profundidade de bico")
plt.show()
```

:::

::: {.column}

```{python}
#| fig-align: center
plt.figure(figsize=(5,3))
sns.scatterplot(
  data=penguins_subset_bill_info,
  x="bill_length_mm",
  y="bill_depth_mm",
  hue="species",
  palette="tab10",
  s=40,
  alpha=0.8
)
plt.title("Distribuição por espécie (bico)")
plt.xlabel("Comprimento de bico")
plt.ylabel("Profundidade de bico")
plt.show()
```

:::

::::

## Agrupando os dados de pinguins (bico) {.smaller}

:::: {.columns}

 - Usando **2** como valor de *k* 

```{python}
k = 2
km = KMeans(n_clusters=k, random_state=42, n_init=10)
penguins_subset_bill_info_pred = km.fit_predict(penguins_subset_bill_info[['bill_length_mm', 'bill_depth_mm']])
penguins_subset_bill_info["kmeans_label"] = penguins_subset_bill_info_pred

centroids = km.cluster_centers_
centroids
```

::::

## Visualizando os grupos (bico) {.smaller}

:::: {.columns}

::: {.column}

```{python}
#| fig-align: center
plt.figure(figsize=(6,4))
plt.scatter(penguins_subset_bill_info["bill_length_mm"], penguins_subset_bill_info["bill_depth_mm"], c=penguins_subset_bill_info["kmeans_label"], cmap="tab10", s=30, alpha=0.8)
plt.scatter(centroids[:,0], centroids[:,1], c="black", s=100, marker="x")
plt.title("K-means (k=2) - clusters e centroides")
plt.xlabel("Comprimento de bico"); plt.ylabel("Profundidade de bico")
plt.show()
```

:::

::: {.column}

```{python}
#| fig-align: center
plt.figure(figsize=(5,3))
sns.scatterplot(
  data=penguins_subset_bill_info,
  x="bill_length_mm",
  y="bill_depth_mm",
  hue="species",
  palette="tab10",
  s=40,
  alpha=0.8
)
plt.title("Distribuição por espécie (bico)")
plt.xlabel("Comprimento de bico")
plt.ylabel("Profundidade de bico")
plt.show()
```

:::

::::

# Análise de Componentes Principais

## Análise de Componentes Principais (PCA)

:::: {.columns}

Técnica de redução de dimensionalidade

Amplamente usada:

 - Análise de dados
 - Aprendizado de máquina

::::

## Redução de dimensionalidade

:::: {.columns}

 - Identificação de componentes ortogonais que capturam a maior parte da variância no conjunto de dados inicial

 - Transformação de um conjunto de **features correlacionadas** em um **conjunto MENOR de features não correlacionadas**

::::


## Componentes Principais {.smaller}

:::: {.columns}

 Features linearmente não correlacionadas, cujos índices estão medindo diferentes dimensões dos dados:

 - **Primeira componente principal (PC1)**: direção no espaço de feature com maior variação dos dados.

 - **Segunda componente principal (PC2)**: direção no espaço de feature com maior variação dos dados, que seja ortogonal à PC1

 - **Demais componentes principais (PC3, PC4 ...)**: direção no espaço de feature com maior variação dos dados, que sejam ortogonais às componentes anteriores.

A maior parte da variação costuma ser descrita pelas primeiras CPs, cujas variâncias não são desprezíveis.

::::


## Quando usar

:::: {.columns}

 - Estão presentes **features correlacionadas** (correlação > 0,30)

 - Explora-se de pelo menos 70% da variância total da amostra 
 
 - É gerado um número mínimo de componentes principais

::::

## Etapas da PCA

:::: {.columns}


 - **Padronização** dos dados (PCA é afetada pela escala das variáveis/features)

 - Geração de uma **matriz de covariância**

 - **Decomposição espectral** - análise de autovalores e autovetores

 - **Ordenação de CPs** de maior para menor autovalor (valores maiores explicam a maior variância)


::::

## Aplicações de PCA

:::: {.columns}


 - **Visualização**

 - **Exploração**

 - **Pré-processamento**

::::

# PCA na prática

## Criando vetores de variáveis em dataframe {.smaller}

:::: {.columns}

```{python}
# gera um DataFrame com 20 variáveis conforme solicitado
rng = np.random.default_rng(12345)
n = 150

g1 = 6   # ~30% de 20
g2 = 8   # ~40% de 20
g3 = 20 - g1 - g2  # restante (6)

# sinais latentes independentes para cada grupo (garante alta correlação intra-grupo e baixa entre grupos)
z1 = rng.normal(0, 1, size=n)
z2 = rng.normal(5, 1.5, size=n)
```

::::

:::: {.columns}

```{python}
# grupo 1: altamente correlacionadas entre si
g1_vars = {f"G1_{i+1}": z1 + rng.normal(0, 0.1, size=n) for i in range(g1)}

# grupo 2: altamente correlacionadas entre si, pouco correlacionadas com grupo 1
g2_vars = {f"G2_{i+1}": z2 + rng.normal(0, 0.1, size=n) for i in range(g2)}

# variáveis restantes com distribuições aleatórias variadas
g3_vars = {
  "R_1": rng.uniform(-2, 2, size=n),
  "R_2": rng.exponential(1.0, size=n),
  "R_3": rng.poisson(3, size=n),
  "R_4": rng.normal(0, 1, size=n),
  "R_5": rng.binomial(1, 0.3, size=n),
  "R_6": rng.normal(2, 0.5, size=n),
}
```

::::

## Criando vetores de variáveis em dataframe {.smaller}

:::: {.columns}

```{python}
data = {**g1_vars, **g2_vars, **g3_vars}
df_pca = pd.DataFrame(data)
df_pca.head()
```

::::

## Criando vetores de variáveis em dataframe {.smaller}

:::: {.columns}

```{python}
df_pca.shape
```

::::

## Padronizando os dados {.smaller}

:::: {.columns}

```{python}
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_pca)
df_pca_scaled = pd.DataFrame(X_scaled, columns=df_pca.columns, index=df_pca.index)
df_pca_scaled.head()
```

::::

## Padronizando os dados {.smaller}

:::: {.columns}

```{python}
df_pca_scaled.shape
```

::::

## Calculando a matriz de covariância {.smaller}

:::: {.columns}

```{python}
cov_matrix = df_pca_scaled.cov()
cov_matrix.head(n = 8)
```

::::

## Decomposição espectral

 - **Autovalores**: variância explicada pelo componente
 
 - **Autovetores**: direção do componente

## Realizando a decomposição espectral - autovetores e autovalores {.smaller}

:::: {.columns}

::: {.column}

```{python}
import scipy as sp

eigvals, eigvecs = sp.linalg.eigh(cov_matrix)

# ordenar autovalores/autovetores em ordem decrescente
order = np.argsort(eigvals)[::-1]
eigvals = eigvals[order]
eigvecs = eigvecs[:, order]
```

:::

::::

## Autovetores e autovalores {.smaller}

:::: {.columns}

::: {.column}

```{python}
eigvals
```

:::

::: {.column}

```{python}
eigvecs
```

:::

::::

## Variância explicada {.smaller}

:::: {.columns}

```{python}
explained_var = eigvals / eigvals.sum()
explained_var
```

::::

## Obtendo a variância cumulativa (ordenando os PCs) {.smaller}

:::: {.columns}

```{python}
cum_explained = np.cumsum(explained_var)

explained_df = pd.DataFrame({
  "eigenvalue": eigvals,
  "explained_variance": explained_var,
  "cumulative_variance": cum_explained
}, index=[f"PC{i+1}" for i in range(len(eigvals))])

explained_df.head()
```

::::

## Projeção dos dados padronizados nas componentes principais {.smaller}

:::: {.columns}

```{python}
X_scaled
```

::::

## Projeção dos dados padronizados nas componentes principais {.smaller}

:::: {.columns}

```{python}
pcs = pd.DataFrame(X_scaled.dot(eigvecs), columns=explained_df.index, index=df_pca.index)
pcs.head()
```

::::

## Projeção dos dados padronizados nas componentes principais {.smaller}

:::: {.columns}

```{python}
#| fig-align: center
fig, axes = plt.subplots(1, 2, figsize=(10, 3))
axes[0].bar(range(1, len(explained_var)+1), explained_var * 100)
axes[0].set_xlabel("PC")
axes[0].set_ylabel("% Variância explicada")
axes[0].set_title("Scree plot")

axes[1].plot(range(1, len(cum_explained)+1), cum_explained * 100, marker='o')
axes[1].axhline(70, color='gray', ls='--')
axes[1].set_xlabel("Número de PCs")
axes[1].set_ylabel("Variância acumulada (%)")
axes[1].set_title("Variância acumulada")

plt.tight_layout()
plt.show()
```

::::

## Projeção dos dados padronizados nas componentes principais {.smaller}

:::: {.columns}

```{python}
#| fig-align: center
sns.scatterplot(x="PC1", y="PC2", data=pcs)
plt.title("PC1 vs PC2")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.show()
```

::::

# Análises de componentes principais com o dataset dos pinguins {.smaller}

## Dataset pinguins

:::: {.columns}

Importando dataset de pinguins:

```{python}
from palmerpenguins import load_penguins
penguins = load_penguins()
penguins.head()
```

::::

## Removendo dados faltantes para PCA {.smaller}

:::: {.columns}

```{python}
penguins_pca = penguins[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']].dropna()

penguins_pca.head()
```

::::

## Padronizando os dados para PCA {.smaller}

:::: {.columns}

::: {.column}

```{python}
scaler = StandardScaler()
X_scaled = scaler.fit_transform(penguins_pca)

df_pca_scaled = pd.DataFrame(X_scaled, columns=penguins_pca.columns)
df_pca_scaled.head()
```

:::

::: {.column}

```{python}
cov_matrix = df_pca_scaled.cov()
eigvals, eigvecs = sp.linalg.eigh(cov_matrix)

print(eigvals)
print(eigvecs)
print(cov_matrix)
```

:::

::::

## PCA com dados de pinguins {.smaller}

:::: {.columns}

```{python}
order = np.argsort(eigvals)[::-1]
eigvals = eigvals[order]
eigvecs = eigvecs[:, order]

explained_var = eigvals / eigvals.sum()
cum_explained = np.cumsum(explained_var)

explained_df = pd.DataFrame({
  "eigenvalue": eigvals,
  "explained_variance": explained_var,
  "cumulative_variance": cum_explained
}, index=[f"PC{i+1}" for i in range(len(eigvals))])

explained_df.head()
```

::::

## Visualizando PCAs {.smaller}

:::: {.columns}

```{python}
#| fig-align: center
pcs = pd.DataFrame(X_scaled.dot(eigvecs), columns=explained_df.index)

fig, axes = plt.subplots(1, 2, figsize=(10, 4))
axes[0].bar(range(1, len(explained_var)+1), explained_var * 100)
axes[0].set_xlabel("PC")
axes[0].set_ylabel("% Variância explicada")
axes[0].set_title("Scree plot")

axes[1].plot(range(1, len(cum_explained)+1), cum_explained * 100, marker='o')
axes[1].axhline(70, color='gray', ls='--')
axes[1].set_xlabel("Número de PCs")
axes[1].set_ylabel("Variância acumulada (%)")
axes[1].set_title("Variância acumulada")
```

::::

## Visualizando PCAs {.smaller}

:::: {.columns}

```{python}
plt.tight_layout()
plt.show()

plt.figure(figsize=(6, 4))
plt.scatter(pcs["PC1"], pcs["PC2"], alpha=0.6)
plt.xlabel(f"PC1 ({explained_var[0]*100:.1f}%)")
plt.ylabel(f"PC2 ({explained_var[1]*100:.1f}%)")
plt.title("PCA - Pinguins Dataset")
plt.grid(True, alpha=0.3)
plt.show()
```

::::

## Visualizando PCAs {.smaller}

:::: {.columns}

```{python}
#| fig-align: center
penguins_species = penguins.iloc[penguins_pca.index]['species']
penguins_species.head()
```

::::

## Visualizando PCAs {.smaller}

:::: {.columns}

```{python}
#| fig-align: center
plt.figure(figsize=(6, 4))
sns.scatterplot(x="PC1", y="PC2", data=pcs, hue=penguins_species, palette="tab10", s=100, alpha=0.7)
plt.xlabel(f"PC1 ({explained_var[0]*100:.1f}%)")
plt.ylabel(f"PC2 ({explained_var[1]*100:.1f}%)")
plt.title("PCA - Pinguins Dataset by Species")
plt.grid(True, alpha=0.3)
plt.show()
```

::::

## Visualizando PCAs {.smaller}

 * Correção de índice para alinhar espécies com os dados

:::: {.columns}

```{python}
penguins_pca = penguins[['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']].dropna()

scaler = StandardScaler()
X_scaled = scaler.fit_transform(penguins_pca)

# IMPORTANT: keep the same index as penguins_pca
pcs = pd.DataFrame(
    X_scaled.dot(eigvecs),
    index=penguins_pca.index,
    columns=explained_df.index
)

# species aligned by label, not by position
penguins_species = penguins.loc[penguins_pca.index, 'species']
```

::::

## Visualizando PCAs {.smaller}

:::: {.columns}

```{python}
plt.figure(figsize=(6, 4))
sns.scatterplot(x="PC1", y="PC2", data=pcs, hue=penguins_species,
                palette="tab10", s=100, alpha=0.7)
plt.xlabel(f"PC1 ({explained_var[0]*100:.1f}%)")
plt.ylabel(f"PC2 ({explained_var[1]*100:.1f}%)")
plt.title("PCA - Penguins Dataset by Species")
plt.grid(True, alpha=0.3)
plt.show()
```

::::

# Uso de Inteligência Artificial

Material com exemplos criados com GitHub Copilot usando modelo(s):

 - `GPT 5.0`


# Referências

 - [PCA with penguins and recipes](https://allisonhorst.github.io/palmerpenguins/articles/pca.html)

 - Géron, A. Hands-On Machine Learning with Scikit-Learn, Keras & Tensorflow. Terceira Edição (capítulo 9). 2025.

 - Munonye, K. Data Science and Analytics with Python: A Comprehensive Guide. 2025.

 - Machado, C. C. e Campos, F. K. R. Análise multivariada: introdução aos conceitos. 2023.

